{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {},
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market - 6 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {},
   "source": [
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy π∗ using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4feac",
   "metadata": {},
   "source": [
    "### --- 1. Custom Environment Creation (SmartSupplierEnv) --- ( 1 Mark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a4a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------\n",
    "# 1. Define Environment\n",
    "# ------------------------\n",
    "\n",
    "# Raw material per day\n",
    "MAX_RM = 10\n",
    "\n",
    "# Market states\n",
    "MARKET_STATES = {\n",
    "    1: {'price_A': 8, 'price_B': 2},   # High Demand A\n",
    "    2: {'price_A': 3, 'price_B': 5},   # High Demand B\n",
    "}\n",
    "\n",
    "# Actions: (units of A, units of B) and their RM cost\n",
    "ACTIONS = {\n",
    "    'Produce_2A_0B': (2, 0),\n",
    "    'Produce_1A_2B': (1, 2),\n",
    "    'Produce_0A_5B': (0, 5),\n",
    "    'Produce_3A_0B': (3, 0),\n",
    "    'Do_Nothing':     (0, 0),\n",
    "}\n",
    "ACTION_LIST = list(ACTIONS.keys())\n",
    "\n",
    "# Horizon: number of days\n",
    "HORIZON = 5\n",
    "\n",
    "# Transition probability of market state (50/50)\n",
    "TRANSITION_PROB = {1: 0.5, 2: 0.5}\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# 2. Reward Function\n",
    "# ------------------------\n",
    "def reward(day, rm, market_state, action):\n",
    "    \"\"\"\n",
    "    Compute immediate reward for taking `action` in given (day, rm, market_state).\n",
    "    \"\"\"\n",
    "    units_A, units_B = ACTIONS[action]\n",
    "    cost_rm = 2*units_A + 1*units_B\n",
    "    # If not enough raw material, action fails -> zero production\n",
    "    if cost_rm > rm:\n",
    "        units_A, units_B = 0, 0\n",
    "    pA = MARKET_STATES[market_state]['price_A']\n",
    "    pB = MARKET_STATES[market_state]['price_B']\n",
    "    return units_A * pA + units_B * pB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2487",
   "metadata": {},
   "source": [
    "### --- 2. Dynamic Programming Implementation (Value Iteration or Policy Iteration) --- (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "027db857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 3. Finite‑Horizon Value Iteration\n",
    "# ------------------------\n",
    "# V[t][rm][m] = max expected cumulative reward from day t..HORIZON starting\n",
    "V = np.zeros((HORIZON+2, MAX_RM+1, len(MARKET_STATES)+1))\n",
    "# π[t][rm][m] = best action at (t,rm,m)\n",
    "policy = {}\n",
    "\n",
    "# Initialize V at day HORIZON+1 to zero (no future days)\n",
    "for t in reversed(range(1, HORIZON+1)):\n",
    "    for rm in range(0, MAX_RM+1):\n",
    "        for m in MARKET_STATES:\n",
    "            best_val = -np.inf\n",
    "            best_act = None\n",
    "            # Evaluate each action\n",
    "            for act in ACTION_LIST:\n",
    "                r = reward(t, rm, m, act)\n",
    "                # Next day resets rm to MAX_RM; market transitions randomly\n",
    "                exp_future = 0.0\n",
    "                for m_next, p in TRANSITION_PROB.items():\n",
    "                    exp_future += p * V[t+1, MAX_RM, m_next]\n",
    "                total = r + exp_future\n",
    "                if total > best_val:\n",
    "                    best_val = total\n",
    "                    best_act = act\n",
    "            V[t, rm, m] = best_val\n",
    "            policy[(t, rm, m)] = best_act"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfa71",
   "metadata": {},
   "source": [
    "#### --- 3. Simulation and Policy Analysis ---  ( 1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62490896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal state‑value V*(day=1, RM=10, market=1): 122.00\n",
      "\n",
      "Day 1, RM=10, Market 1 → Produce: Produce_3A_0B\n",
      "Day 1, RM=10, Market 2 → Produce: Produce_0A_5B\n",
      "\n",
      "Day 3, RM=10, Market 1 → Produce: Produce_3A_0B\n",
      "Day 3, RM=10, Market 2 → Produce: Produce_0A_5B\n",
      "\n",
      "Day 5, RM=10, Market 1 → Produce: Produce_3A_0B\n",
      "Day 5, RM=10, Market 2 → Produce: Produce_0A_5B\n",
      "\n",
      "Average total profit over 1000 runs: 122.55\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# 4. Inspect Optimal Policy & Value\n",
    "# ------------------------\n",
    "start_value = V[1, MAX_RM, 1]\n",
    "print(f\"Optimal state‑value V*(day=1, RM=10, market=1): {start_value:.2f}\\n\")\n",
    "\n",
    "# Show policy for a few sample states\n",
    "for day in [1, 3, 5]:\n",
    "    for m in [1, 2]:\n",
    "        act = policy[(day, MAX_RM, m)]\n",
    "        print(f\"Day {day}, RM=10, Market {m} → Produce: {act}\")\n",
    "    print()\n",
    "\n",
    "# ------------------------\n",
    "# 5. Simulate the Learned Policy\n",
    "# ------------------------\n",
    "def run_episode(policy):\n",
    "    total = 0\n",
    "    rm = MAX_RM\n",
    "    market = random.choice([1, 2])\n",
    "    for t in range(1, HORIZON+1):\n",
    "        act = policy[(t, rm, market)]\n",
    "        r = reward(t, rm, market, act)\n",
    "        total += r\n",
    "        # Next day\n",
    "        rm = MAX_RM\n",
    "        market = 1 if random.random() < 0.5 else 2\n",
    "    return total\n",
    "\n",
    "# Run many episodes to estimate average profit\n",
    "N_EPISODES = 1000\n",
    "profits = [run_episode(policy) for _ in range(N_EPISODES)]\n",
    "avg_profit = np.mean(profits)\n",
    "print(f\"Average total profit over {N_EPISODES} runs: {avg_profit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {},
   "source": [
    "#### --- 4. Impact of Dynamics Analysis --- (1 Mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4929e-e911-4d91-8918-854fa7e54f75",
   "metadata": {},
   "source": [
    "# Discusses the impact of dynamic market prices on the optimal policy.\n",
    "# This section should primarily be a written explanation in the report.\n",
    "When market prices for products A and B fluctuate over time, a static “one‑size‑fits‑all” production plan quickly becomes suboptimal. Dynamic programming (DP), and in particular finite‑horizon value iteration, lets the supplier tailor its daily production decisions to the current price regime while still accounting for future uncertainties. Here’s how:\n",
    "\n",
    "(1) Price‑Driven Action Selection\n",
    "\n",
    "High‑Price States: On days when product A’s price spikes (e.g., state 1: $8 for A vs. $2 for B), the DP‑derived policy will allocate as much raw material as feasible toward A, even if that means foregoing B completely. Conversely, when B’s price is higher (state 2), the policy pivots to produce B.\n",
    "\n",
    "Threshold Behavior: Because production of A consumes twice as much raw material per unit as B, the policy often exhibits a “threshold” rule: only when A’s price advantage exceeds a certain margin does it choose the higher‑cost, higher‑reward option.\n",
    "\n",
    "(2) Balancing Immediate vs. Future Reward\n",
    "\n",
    "Finite Horizon Tradeoff: DP explicitly balances today’s profit against expected profits in future days. For instance, if tomorrow’s market is very likely to flip back to a state favoring A, the policy may “save” some raw material today (by producing fewer units or even doing nothing) in order to reap higher returns later.\n",
    "\n",
    "Adaptive Conservatism: In practice, this yields adaptive conservatism—moderate production on marginally favorable price days, with heavier production reserved for days forecasted (probabilistically) to be more lucrative.\n",
    "\n",
    "(3) Robustness to Price Volatility\n",
    "\n",
    "Stochastic Averaging: By incorporating the 50/50 transition probabilities into its Bellman updates, DP smooths out erratic price swings. Actions are chosen not just for the current price but for how they buffer risk across all future price paths.\n",
    "\n",
    "Policy Stability: Even if actual day‑to‑day price realizations differ from expectations, the DP policy remains near‑optimal, since it was optimized over the full distribution of possible future states.\n",
    "\n",
    "(4) Operational Insights\n",
    "\n",
    "Spot‑Price Exploitation: The DP solution highlights when “spot plays” (i.e., producing only on peak‑price days) are optimal versus maintaining a steady production rhythm.\n",
    "\n",
    "Inventory Valuation: In settings where raw material can carry over (if modified), the same DP framework would assign an “option value” to holding inventory when both products are temporarily undervalued.\n",
    "\n",
    "(5) Economic Value of Flexibility\n",
    "\n",
    "Quantified Gains: Simulations typically show that a DP‑based policy yields significantly higher average profit than a naïve fixed rule (e.g., always produce two units of A), because it systematically exploits favorable price states and avoids overproduction in unfavorable ones.\n",
    "\n",
    "Decision Support: Managers can use the value function surface V(t,RM,m) to quantify how much an extra unit of raw material is worth in each market state and time, guiding not only production but also procurement and pricing strategies.\n",
    "\n",
    "In summary, dynamic programming transforms a volatile pricing environment from a risk‑laden guessing game into a structured optimization problem. By embedding market‑state transitions and finite‑horizon goals into the Bellman equations, DP yields an optimal policy that dynamically reallocates resources to wherever they earn the highest expected return—both today and tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e6570fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal V*(day=1, RM=10, market=1): 122.00\n",
      "\n",
      " Day 1, RM=10, Market 1 → Produce_3A_0B\n",
      " Day 1, RM=10, Market 2 → Produce_0A_5B\n",
      "\n",
      " Day 3, RM=10, Market 1 → Produce_3A_0B\n",
      " Day 3, RM=10, Market 2 → Produce_0A_5B\n",
      "\n",
      " Day 5, RM=10, Market 1 → Produce_3A_0B\n",
      " Day 5, RM=10, Market 2 → Produce_0A_5B\n",
      "\n",
      "Avg. profit over 1000 runs: 122.57\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "import random\n",
    "\n",
    "# ------------------------\n",
    "# 1. Define Environment\n",
    "# ------------------------\n",
    "MAX_RM = 10\n",
    "MARKET_STATES = {\n",
    "    1: {'price_A': 8, 'price_B': 2},\n",
    "    2: {'price_A': 3, 'price_B': 5},\n",
    "}\n",
    "ACTIONS = {\n",
    "    'Produce_2A_0B': (2, 0),\n",
    "    'Produce_1A_2B': (1, 2),\n",
    "    'Produce_0A_5B': (0, 5),\n",
    "    'Produce_3A_0B': (3, 0),\n",
    "    'Do_Nothing':     (0, 0),\n",
    "}\n",
    "ACTION_LIST = list(ACTIONS.keys())\n",
    "HORIZON = 5\n",
    "TRANSITION_PROB = {1: 0.5, 2: 0.5}\n",
    "\n",
    "# ------------------------\n",
    "# 2. Reward Function\n",
    "# ------------------------\n",
    "def reward(rm, market_state, action):\n",
    "    uA, uB = ACTIONS[action]\n",
    "    cost = 2*uA + 1*uB\n",
    "    if cost > rm:\n",
    "        uA, uB = 0, 0\n",
    "    pA = MARKET_STATES[market_state]['price_A']\n",
    "    pB = MARKET_STATES[market_state]['price_B']\n",
    "    return uA*pA + uB*pB\n",
    "\n",
    "# ------------------------\n",
    "# 3. Initialize Value & Policy Tables\n",
    "# ------------------------\n",
    "# V[t][rm][m] = 0 for all t,rm,m\n",
    "V = [\n",
    "    [ {1:0.0, 2:0.0} for _ in range(MAX_RM+1) ]\n",
    "    for _ in range(HORIZON+2)\n",
    "]\n",
    "policy = {}  # (t,rm,m) -> best action\n",
    "\n",
    "# ------------------------\n",
    "# 4. Finite‑Horizon Value Iteration\n",
    "# ------------------------\n",
    "for t in range(HORIZON, 0, -1):\n",
    "    for rm in range(MAX_RM+1):\n",
    "        for m in MARKET_STATES:\n",
    "            best_val = float('-inf')\n",
    "            best_act = None\n",
    "            for act in ACTION_LIST:\n",
    "                r = reward(rm, m, act)\n",
    "                # next-day RM resets to MAX_RM\n",
    "                exp_future = sum(\n",
    "                    TRANSITION_PROB[m_next] * V[t+1][MAX_RM][m_next]\n",
    "                    for m_next in MARKET_STATES\n",
    "                )\n",
    "                total = r + exp_future\n",
    "                if total > best_val:\n",
    "                    best_val = total\n",
    "                    best_act = act\n",
    "            V[t][rm][m] = best_val\n",
    "            policy[(t, rm, m)] = best_act\n",
    "\n",
    "# ------------------------\n",
    "# 5. Inspect Results\n",
    "# ------------------------\n",
    "print(f\"Optimal V*(day=1, RM=10, market=1): {V[1][10][1]:.2f}\\n\")\n",
    "for day in [1,3,5]:\n",
    "    for m in [1,2]:\n",
    "        print(f\" Day {day}, RM=10, Market {m} → {policy[(day,10,m)]}\")\n",
    "    print()\n",
    "\n",
    "# ------------------------\n",
    "# 6. Simulate Policy\n",
    "# ------------------------\n",
    "def run_episode():\n",
    "    total = 0\n",
    "    rm = MAX_RM\n",
    "    market = random.choice([1,2])\n",
    "    for t in range(1, HORIZON+1):\n",
    "        act = policy[(t, rm, market)]\n",
    "        total += reward(rm, market, act)\n",
    "        rm = MAX_RM\n",
    "        market = 1 if random.random() < 0.5 else 2\n",
    "    return total\n",
    "\n",
    "# Estimate average profit\n",
    "trials = 1000\n",
    "profits = [run_episode() for _ in range(trials)]\n",
    "print(f\"Avg. profit over {trials} runs: {sum(profits)/trials:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
